2025-11-04 20:40:38,418 - numexpr.utils - INFO - Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
2025-11-04 20:40:38,544 - numexpr.utils - INFO - Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
2025-11-04 20:40:38,544 - numexpr.utils - INFO - NumExpr defaulting to 16 threads.
2025-11-04 20:40:39,479 - datasets - INFO - PyTorch version 2.7.1+cu118 available.
2025-11-04 20:40:40,210 - utils.log - INFO - Start print log
2025-11-04 20:40:40,213 - utils.log - INFO - torch:2.7.1+cu118
2025-11-04 20:40:40,218 - utils.log - INFO - transformers:4.57.1
2025-11-04 20:40:40,218 - utils.log - INFO - accelerate:1.11.0
2025-11-04 20:40:40,219 - utils.log - INFO - Namespace(model='meta-llama/Meta-Llama-3-8B', seed=0, dataset='wikitext2', nsamples=128, calibration_seqlen=128, eval_seqlen=128, sparsity_ratio=0.5, cache_dir='llm_weights', eval_results='result/eval', save_model='pruned_model/lorap_0.2/', para_allocate=3.0, mlp_compress_method='prune', mlp_least_save_ratio=0.01, real_com=True, deco_method='AWSVD', sublayer='mlp')
2025-11-04 20:40:41,878 - utils.log - INFO - tokenizer load finished
2025-11-04 20:40:43,662 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-11-04 20:40:47,619 - utils.log - INFO - model load finished
